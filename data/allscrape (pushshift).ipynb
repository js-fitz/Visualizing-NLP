{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import calendar\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get 1 month of subreddit data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month_data(month, year, subreddit): # for subsim2015 (<1000 posts/month)\n",
    "    start=int(dt.datetime(year, month, 1).timestamp())\n",
    "    end=int(dt.datetime(year, month, calendar.monthrange(year,month)[1]).timestamp())\n",
    "    uri = 'https://apiv2.pushshift.io/reddit/submission/search/?subreddit={}&limit={}&before={}&after={}'.format(\\\n",
    "             subreddit, 1000, end, start)\n",
    "    res = requests.get(uri)\n",
    "    res = pd.DataFrame(res.json()['data'])\n",
    "    return res\n",
    "    return res_df\n",
    "\n",
    "def get_month_data_limited(month, year, subreddit): # for gpt2 (>1000 posts/month)\n",
    "    res = pd.DataFrame()\n",
    "    for third in [(1, 10), (11,20), (20, calendar.monthrange(year,month)[1])]:\n",
    "        start=int(dt.datetime(year, month, third[0]).timestamp())\n",
    "        end=int(dt.datetime(year, month, third[1]).timestamp())\n",
    "        uri = 'https://apiv2.pushshift.io/reddit/submission/search/?subreddit={}&limit={}&before={}&after={}'.format(\\\n",
    "                 subreddit, 1000, end, start)\n",
    "        res_ = requests.get(uri)\n",
    "        res = pd.concat([res, pd.DataFrame(res_.json()['data'])], sort=False)\n",
    "        time.sleep(1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get all subreddit data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_subreddit(subreddit, start_year, lim=False):\n",
    "    scrapeddit = pd.DataFrame()\n",
    "    for year in range(start_year, 2020):\n",
    "        for month in range(1,13):\n",
    "            if lim==False: month_data = get_month_data(month, year, subreddit)\n",
    "            if lim==True: month_data = get_month_data_limited(month, year, subreddit) # for gpt2\n",
    "            scrapeddit = pd.concat([scrapeddit, month_data], sort=False)\n",
    "            print(f'{year} {month} added ({len(month_data)} rows)')\n",
    "            time.sleep(1)\n",
    "    month_data = get_month_data(1, 2020, subreddit) # add jan 2020\n",
    "    print(f'2020 1 added ({len(month_data)} rows)')\n",
    "    return pd.concat([scrapeddit, month_data], sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# `scrape:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015 1 added (0 rows)\n",
      "2015 2 added (0 rows)\n",
      "2015 3 added (0 rows)\n",
      "2015 4 added (0 rows)\n",
      "2015 5 added (0 rows)\n",
      "2015 6 added (521 rows)\n",
      "2015 7 added (688 rows)\n",
      "2015 8 added (673 rows)\n",
      "2015 9 added (662 rows)\n",
      "2015 10 added (524 rows)\n",
      "2015 11 added (567 rows)\n",
      "2015 12 added (598 rows)\n",
      "2016 1 added (594 rows)\n",
      "2016 2 added (565 rows)\n",
      "2016 3 added (577 rows)\n",
      "2016 4 added (628 rows)\n",
      "2016 5 added (622 rows)\n",
      "2016 6 added (568 rows)\n",
      "2016 7 added (604 rows)\n",
      "2016 8 added (632 rows)\n",
      "2016 9 added (636 rows)\n",
      "2016 10 added (662 rows)\n",
      "2016 11 added (621 rows)\n",
      "2016 12 added (638 rows)\n",
      "2017 1 added (624 rows)\n",
      "2017 2 added (538 rows)\n",
      "2017 3 added (577 rows)\n",
      "2017 4 added (574 rows)\n",
      "2017 5 added (632 rows)\n",
      "2017 6 added (593 rows)\n",
      "2017 7 added (662 rows)\n",
      "2017 8 added (676 rows)\n",
      "2017 9 added (624 rows)\n",
      "2017 10 added (658 rows)\n",
      "2017 11 added (627 rows)\n",
      "2017 12 added (626 rows)\n",
      "2018 1 added (619 rows)\n",
      "2018 2 added (559 rows)\n",
      "2018 3 added (609 rows)\n",
      "2018 4 added (599 rows)\n",
      "2018 5 added (604 rows)\n",
      "2018 6 added (572 rows)\n",
      "2018 7 added (582 rows)\n",
      "2018 8 added (574 rows)\n",
      "2018 9 added (560 rows)\n",
      "2018 10 added (526 rows)\n",
      "2018 11 added (496 rows)\n",
      "2018 12 added (522 rows)\n",
      "2019 1 added (511 rows)\n",
      "2019 2 added (506 rows)\n",
      "2019 3 added (525 rows)\n",
      "2019 4 added (506 rows)\n",
      "2019 5 added (568 rows)\n",
      "2019 6 added (557 rows)\n",
      "2019 7 added (535 rows)\n",
      "2019 8 added (535 rows)\n",
      "2019 9 added (506 rows)\n",
      "2019 10 added (511 rows)\n",
      "2019 11 added (515 rows)\n",
      "2019 12 added (505 rows)\n",
      "2020 1 added (516 rows)\n",
      " completed in 240.36277029299998 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "subsim = scrape_subreddit('subredditsimulator', 2015)\n",
    "print(f' completed in {time.perf_counter() - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019 1 added (0 rows)\n",
      "2019 2 added (0 rows)\n",
      "2019 3 added (0 rows)\n",
      "2019 4 added (0 rows)\n",
      "2019 5 added (0 rows)\n",
      "2019 6 added (1391 rows)\n",
      "2019 7 added (2061 rows)\n",
      "2019 8 added (1543 rows)\n",
      "2019 9 added (1202 rows)\n",
      "2019 10 added (1385 rows)\n",
      "2019 11 added (1344 rows)\n",
      "2019 12 added (1384 rows)\n",
      "2020 1 added (1000 rows)\n",
      " completed in 118.71145535800008 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "gpt2 = scrape_subreddit('subsimulatorgpt2', 2019, lim=True)\n",
    "print(f' completed in {time.perf_counter() - start} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsim.to_csv('data/ss_data.csv', index=False)\n",
    "gpt2.to_csv('data/gp_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
